# -*- coding: utf-8 -*-
"""Question_Answering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wPn4HVVVrx2MktkUB0K9Za2-U5ceBipC
"""

from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import DefaultDataCollator
from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer
import torch

#Load in the data set. In this case, hugging face suggested us the squad dataset. I shortened the training model to only 3000 rows to make it smaller.
squad = load_dataset("squad", split="train[:3000]")

#Chose to use a 60% train and a 40% test for the task
squad = squad.train_test_split(test_size=0.4)

#Used the most downloaded pre trained model for question and answer
tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")

#Preprocess function as suggested by hugging face
def preprocess_function(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=384,
        truncation="only_second",
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        answer = answers[i]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])
        sequence_ids = inputs.sequence_ids(i)

        # Find the start and end of the context
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # If the answer is not fully inside the context, label it (0, 0)
        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Otherwise it's the start and end token positions
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs

#Map the preprocess
tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)

# Define the model
model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")

#Define the question and answer below
question = "What is unique about Norway?"
context = "Norway is a country of breathtaking glaciers, fjords, and avid winter sport enthusiasts. The terrain is glaciated with mostly high plateaus and rugged mountains broken by fertile valleys, scattered plains, coastline deeply indented by fjords, and arctic tundra in north. During the warmer months, Norwegians of all ages love to be outside and hike, fish, and barbecue. In the colder months, some travelers are lucky enough to catch a glimpse of the spectacular Aurora Borealis (The Northern Lights). Norwegians tend to have a strong sense of history and civic engagement and on special occasions, many Norwegians wearing traditional clothing, or bunad. In Norwegian culture, some of the most important values are tolerance, respect and equality."

def generate_answer(cont, quest):
  #AutoTokenize and turn into inputs
  tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")
  inputs = tokenizer(quest, cont, return_tensors="pt")

  inputs["input_ids"] = inputs["input_ids"][:,:512]
  inputs["attention_mask"] = inputs["attention_mask"][:,:512]

  #Create outputs using the model with the inputs
  with torch.no_grad():
    outputs = model(**inputs)

  #Get the highest probability from the model output for the start and end positions
  answer_start_index = outputs.start_logits.argmax()
  answer_end_index = outputs.end_logits.argmax()

  #Decode the predicted tokens to get the answer
  predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]

  return tokenizer.decode(predict_answer_tokens)

print(generate_answer(context,question))